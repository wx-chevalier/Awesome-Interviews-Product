计算机视觉核心算法（图像分类/目标检测/图像分割）面试题汇总

一、图像分类相关面试题

基础概念类

- 请简述图像分类的定义和核心任务是什么？
  答案：图像分类是计算机视觉的基础任务，指给定一张输入图像，模型通过学习图像的视觉特征，判断该图像所属的预定义类别（如猫、狗、汽车等）。核心任务是建立从图像像素信息到类别标签的映射关系，本质是对图像进行高层语义理解和归类，为后续更复杂的 CV 任务（如目标检测、分割）提供基础。

- 图像分类任务中常用的评估指标有哪些？请分别解释其计算方式和适用场景（如准确率、精确率、召回率、F1 分数、混淆矩阵、Top-1/Top-5 准确率）。
  答案：常用评估指标及说明如下：

1. 准确率（Accuracy）：计算方式为“正确分类的样本数/总样本数”；适用场景：数据集类别分布均衡，且各类别误判代价相近（如普通物体识别）。局限性：类别不平衡时会失真（如负样本占 99%，模型全判负样本也能得 99%准确率）。
2. 精确率（Precision，查准率）：计算方式为“真阳性（TP）/（真阳性+假阳性（FP））”；适用场景：关注“预测为正类的样本中实际为正类的比例”，避免误判（如垃圾邮件识别，避免将正常邮件归为垃圾邮件）。
3. 召回率（Recall，查全率）：计算方式为“真阳性（TP）/（真阳性+假阴性（FN））”；适用场景：关注“所有实际为正类的样本中被正确识别的比例”，避免漏判（如疾病诊断，避免遗漏患病患者）。
4. F1 分数：计算方式为“2× 精确率 × 召回率/（精确率+召回率）”；适用场景：需要平衡精确率和召回率，解决二者此消彼长的问题（如电商商品分类，既需减少误判也需减少漏判）。
5. 混淆矩阵（Confusion Matrix）：以矩阵形式展示“实际类别-预测类别”的样本分布，行代表实际类别，列代表预测类别；适用场景：全面分析模型在各类别上的表现，定位具体类别（如少数类）的误判问题。
6. Top-1 准确率：预测概率最高的类别与真实类别一致的样本数/总样本数；适用场景：对分类精度要求严格的场景（如普通图像识别竞赛）。
7. Top-5 准确率：预测概率前 5 的类别中包含真实类别的样本数/总样本数；适用场景：高维类别或复杂场景（如 ImageNet 数据集 1000 类分类），允许模型存在一定的预测容错性。

- 什么是过拟合？在图像分类模型训练中，常见的防止过拟合的方法有哪些？请详细说明至少 3 种方法的原理。
  答案：1. 过拟合定义：模型在训练集上表现优异（训练误差低），但在未见过的测试集上表现差（泛化误差高），本质是模型过度学习了训练集中的噪声和个性化特征，而非数据的通用规律。

2. 常见防过拟合方法及原理：
   （1）数据增强（Data Augmentation）：通过对训练图像进行随机变换（如翻转、裁剪、缩放、色域变换等），生成大量“新的、相似的”训练样本。原理：扩大训练集规模，增加数据多样性，迫使模型学习更通用的视觉特征，而非依赖单一样本的噪声。
   （2）正则化（Regularization）：如 L1 正则化、L2 正则化（权重衰减）。原理：在损失函数中加入模型权重的惩罚项（L1 是权重绝对值之和，L2 是权重平方和），限制权重的绝对值大小，避免模型参数过度复杂，减少对训练数据噪声的拟合。
   （3）Dropout：训练时随机使网络中部分神经元的输出为 0（关闭），测试时恢复所有神经元并将输出乘以保留概率（或训练时缩放）。原理：防止神经元过度依赖彼此的协同作用，迫使模型学习更鲁棒的独立特征，模拟“多个子模型集成”的效果，提升泛化能力。
   （4）早停（Early Stopping）：训练过程中实时监控验证集性能，当验证集误差连续多轮不再下降（甚至上升）时，停止训练。原理：避免模型在训练后期过度拟合训练集，保存泛化能力最优的模型参数。
   （5）批量归一化（Batch Normalization）：对每层输入进行标准化（均值为 0、方差为 1），并引入可学习的缩放和偏移参数。原理：缓解“梯度消失/爆炸”，加速训练收敛，同时减少层间参数的相互依赖（降低内部协变量偏移），间接提升模型泛化能力。

- 请解释图像分类中的数据增强技术，列举 5 种常用的数据增强方法，并说明其对模型训练的作用。
  答案：1. 数据增强定义：在不改变图像核心语义（类别）的前提下，对训练图像进行随机、合理的变换，生成更多样的训练样本，是提升模型泛化能力的关键技术之一。

2. 5 种常用方法及作用：
   （1）随机翻转（Horizontal/Vertical Flip）：水平翻转（如将猫的图像左右翻转）或垂直翻转（较少用，适用于无上下差异的图像如卫星图）。作用：打破图像的“方向依赖性”，如模型不会认为“猫只能朝左”，增强对方向变化的鲁棒性。
   （2）随机裁剪（Random Crop）：从原始图像中随机截取部分区域作为训练样本（如 Resize 到 224×224 的图像，先放大到 256×256 再随机裁 224×224）。作用：模拟图像中目标的不同位置、不同尺度，迫使模型关注目标的核心特征而非背景位置。
   （3）随机缩放（Random Scaling）：对图像进行随机放大或缩小后再裁剪到固定尺寸。作用：增强模型对目标尺度变化的适应性，如识别不同大小的同一物体（大汽车、小汽车）。
   （4）色域变换（Color Jitter）：随机调整图像的亮度、对比度、饱和度、色相。作用：模拟不同光照、环境下的图像变化，避免模型过度依赖颜色特征，提升对光照变化的鲁棒性。
   （5）随机旋转（Random Rotation）：将图像随机旋转一定角度（如-15°~15°）。作用：增强模型对目标旋转角度的适应性，如识别倾斜的文字、旋转的物体。
3. 核心作用：扩大训练集规模，增加数据多样性；抑制过拟合，提升模型泛化能力；使模型学习更通用的视觉特征，而非训练数据的个性化噪声。

经典算法原理类

- 请简述 LeNet-5 的网络结构和核心贡献，它在图像分类发展中的意义是什么？
  答案：1. 网络结构：LeNet-5 是针对手写数字识别（MNIST 数据集）设计的卷积神经网络，输入为 32×32×1 的灰度图，共 7 层（含输入层，不含池化层为 5 层，故得名 LeNet-5），结构流程：
  输入层（32×32×1）→ C1 卷积层（6 个 5×5 卷积核，步长 1，输出 28×28×6）→ S2 池化层（2×2 池化核，步长 2，输出 14×14×6）→ C3 卷积层（16 个 5×5 卷积核，步长 1，输出 10×10×16）→ S4 池化层（2×2 池化核，步长 2，输出 5×5×16）→ C5 全连接卷积层（120 个 5×5 卷积核，输出 1×1×120）→ F6 全连接层（84 个神经元）→ 输出层（10 个神经元，Softmax 激活，对应 10 个数字）。

2. 核心贡献：
   （1）首次提出“卷积-池化-全连接”的经典 CNN 架构范式，奠定了现代 CNN 的基础；
   （2）验证了卷积操作在提取局部特征、权值共享（减少参数）的有效性；
   （3）证明了深度学习可用于图像分类任务，突破了传统手工特征的局限。
3. 发展意义：LeNet-5 是第一个成功商业化的 CNN 模型（用于银行支票数字识别），为后续深度学习在计算机视觉领域的爆发积累了实践经验，是图像分类从“手工特征”向“端到端学习”转变的关键里程碑。

- AlexNet 为何能成为深度学习在图像分类领域爆发的标志性模型？请说明其核心创新点（至少 4 点）。
  答案：1. 爆发原因：AlexNet 在 2012 年 ImageNet 图像分类竞赛中，以 Top-5 准确率 84.7%远超传统手工特征方法（第二名 73.8%），差距显著，首次证明了深度学习在大规模图像分类任务中的优越性，彻底改变了计算机视觉领域的研究方向，标志着深度学习在 CV 领域的爆发。

2. 核心创新点（至少 4 点）：
   （1）ReLU 激活函数：替代传统的 Sigmoid/Tanh 激活，解决了深层网络中的梯度消失问题（Sigmoid 在输入绝对值大时梯度趋近于 0），同时 ReLU 计算速度更快（线性操作）。
   （2）Dropout 正则化：随机关闭部分神经元（概率 0.5），防止模型过拟合，提升泛化能力。
   （3）重叠池化（Overlapping Pooling）：池化核大小 3×3，步长 2，相邻池化窗口存在重叠区域，相比传统非重叠池化，能保留更多图像细节，提升特征提取能力。
   （4）数据增强：通过图像翻转、裁剪、色域变换等方法扩大训练集，同时使用 PCA 降维处理图像 RGB 通道，减少光照、颜色变化带来的影响。
   （5）双 GPU 并行训练：将网络分为两部分分别在两个 GPU 上训练，提升训练速度，突破当时硬件算力限制，支持大规模网络训练。
   （6）局部响应归一化（LRN）：对卷积层输出进行归一化，增强模型泛化能力（后续被 BN 替代，但当时是重要创新）。

- 请解释 VGG 网络的“深度加深”思路，其 1x1 卷积和 3x3 卷积的作用分别是什么？
  答案：1. “深度加深”思路：VGG 的核心设计理念是“通过增加网络深度提升模型性能”，在保持网络整体结构简单（仅用 3×3 卷积和 2×2 池化）的前提下，将网络深度从 AlexNet 的 8 层提升到 16-19 层（如 VGG16、VGG19）。思路依据：深层网络能提取更抽象、更复杂的视觉特征（低层特征如边缘、纹理，高层特征如目标部件、整体轮廓），同时通过小尺寸卷积核叠加，在减少参数的同时保证感受野大小。

2. 1×1 卷积的作用：
   （1）维度压缩/升维：在不改变特征图尺寸（步长 1、padding 0）的前提下，调整特征通道数（如将 128 通道压缩到 64 通道），减少计算量；
   （2）特征融合：对同一空间位置的不同通道特征进行线性组合，实现通道间的信息交互；
   （3）引入非线性：配合 ReLU 激活函数，增加网络的非线性表达能力（1×1 卷积本身是线性操作，激活后变为非线性）。
3. 3×3 卷积的作用：
   （1）提取局部特征：3×3 是最小的能捕捉像素上下文信息（上下左右、中心）的卷积核，能有效提取局部纹理、边缘等基础特征；
   （2）减少参数：相比大尺寸卷积核（如 5×5、7×7），多个 3×3 卷积叠加可获得相同感受野，但参数更少（如 2 个 3×3 卷积参数为 2×(3×3×C×C)=18C²，1 个 5×5 卷积参数为 25C²，C 为通道数）；
   （3）加深网络深度：通过叠加 3×3 卷积层，轻松提升网络深度，增强特征提取能力。

- ResNet 是如何解决深层网络训练中的梯度消失/梯度爆炸问题的？请详细说明残差连接（Residual Connection）的原理和结构。
  答案：1. 梯度消失/梯度爆炸原因：深层网络训练时，梯度通过反向传播从输出层向输入层传递，经过多层参数矩阵相乘后，梯度值可能趋近于 0（消失）或趋近于无穷大（爆炸），导致浅层网络参数无法更新，模型无法收敛。

2. ResNet 的核心解决方案：引入残差连接（Residual Connection），通过“跳跃连接”将浅层特征直接传递到深层，使网络能学习“残差映射”（目标映射 H(x) = F(x) + x，其中 F(x)是残差函数，x 是浅层输入特征），而非直接学习目标映射 H(x)，从而缓解梯度问题。
3. 残差连接的原理和结构：
   （1）结构：残差块（Residual Block）是 ResNet 的基本单元，分为“恒等映射块”（输入输出通道数相同，可直接跳跃连接）和“1×1 卷积调整块”（输入输出通道数/尺寸不同，用 1×1 卷积调整 x 的维度后再连接）。
   典型恒等映射块结构：输入 x → 3×3 卷积（ReLU）→ 3×3 卷积 → 残差连接（x 直接加到第二个卷积的输出）→ ReLU；
   维度调整块结构：输入 x → 1×1 卷积（降维，ReLU）→ 3×3 卷积（ReLU）→ 1×1 卷积（升维）→ 残差连接（x 经 1×1 卷积调整维度后相加）→ ReLU。
   （2）原理：

- 梯度传播优化：反向传播时，梯度可通过残差连接直接传递到浅层（梯度 ∂Loss/∂x = ∂Loss/∂H(x) × ∂H(x)/∂x = ∂Loss/∂H(x) × (∂F(x)/∂x + 1)），避免梯度经过多层卷积后衰减为 0（因为“+1”项保证了梯度的基本大小）；
- 学习难度降低：残差函数 F(x) = H(x) - x，当目标映射 H(x)接近恒等映射（深层特征与浅层特征差异不大）时，F(x)接近 0，网络只需学习微小的残差，相比直接学习 H(x)更易收敛；
- 特征复用：浅层特征直接传递到深层，实现不同层级特征的融合，提升模型表达能力。

- DenseNet 的核心思想是什么？与 ResNet 相比，它的优势和不足分别是什么？
  答案：1. 核心思想：DenseNet（密集连接网络）的核心是“密集连接”，即每个卷积层的输出都直接作为后续所有卷积层的输入（形成“稠密连接块”），实现“特征复用”的极致化。具体来说，第 l 层的输入是前 l-1 层所有输出特征图的拼接（通道维度拼接），输出特征图通道数为 k（增长率），网络整体由多个稠密连接块和过渡层（用于降维、降尺寸）组成。
  核心公式：x*l = H_l([x_0, x_1, ..., x*{l-1}])，其中[x_0,...,x_{l-1}]是前 l-1 层特征图的拼接，H_l 是第 l 层的卷积操作（BN+ReLU+3×3 卷积）。

2. 与 ResNet 的优势对比：
   （1）特征复用更充分：ResNet 是“加法融合”（x*l = F_l(x*{l-1}) + x\_{l-1}），DenseNet 是“拼接融合”（所有前层特征直接输入），能更全面地利用不同层级的特征，提升特征表达能力；
   （2）参数效率更高：通过特征复用，DenseNet 用更少的参数（更小的增长率 k）就能达到与 ResNet 相当甚至更优的性能；
   （3）缓解梯度消失更有效：梯度可通过密集连接直接传递到所有浅层，比 ResNet 的单跳跃连接更顺畅；
   （4）抗过拟合能力更强：特征复用增加了数据多样性，同时过渡层的降维操作减少了冗余特征，降低过拟合风险。
3. 不足对比：
   （1）计算量更大：每个层都需要接收前所有层的特征图，拼接后的通道数多，导致卷积计算量增加（尤其是稠密块内部）；
   （2）内存消耗更高：需要同时存储所有前层的特征图，对硬件内存要求更高；
   （3）训练速度更慢：计算量和内存消耗大，导致训练效率低于 ResNet；
   （4）实际部署更复杂：密集连接的结构增加了模型部署的难度（需处理多通道拼接）。

- MobileNet 中的深度可分离卷积（Depthwise Separable Convolution）是如何工作的？它能减少计算量的原理是什么？
  答案：1. 深度可分离卷积的核心目的：为移动设备等资源受限场景设计，在保证模型性能的前提下，大幅减少计算量和参数量，提升推理速度。其核心思想是将传统的“标准卷积”（同时完成通道融合和空间特征提取）拆分为“深度卷积（Depthwise Convolution）”和“逐点卷积（Pointwise Convolution）”两个独立步骤，分步完成特征提取。

2. 工作流程（以输入特征图为 D×D×M，输出为 D×D×N 为例）：
   （1）深度卷积（Depthwise Conv）：为每个输入通道单独分配一个卷积核（共 M 个 3×3 卷积核），每个卷积核只对对应通道进行空间特征提取，输出特征图维度为 D×D×M（通道数不变）。作用：仅完成空间维度的局部特征提取，不进行通道间的融合。
   （2）逐点卷积（Pointwise Conv）：使用 N 个 1×1 卷积核，对深度卷积的输出特征图进行通道维度的融合和调整，输出特征图维度为 D×D×N（通道数变为 N）。作用：完成通道间的信息交互，调整输出通道数，匹配后续网络需求。
3. 减少计算量的原理：通过拆分步骤，避免了标准卷积中“通道融合+空间提取”的冗余计算，具体计算量对比：
   （1）标准卷积计算量：3×3×M×N×D×D（卷积核尺寸 × 输入通道 × 输出通道 × 特征图尺寸）；
   （2）深度可分离卷积计算量：3×3×M×D×D（深度卷积） + 1×1×M×N×D×D（逐点卷积）；
   （3）计算量减少比例：约为 1/N + 1/(3×3) ≈ 1/9（当 N 较大时，1/N 可忽略），即计算量仅为标准卷积的 1/9 左右，大幅降低了硬件计算压力。

- Vision Transformer（ViT）是如何将 Transformer 架构应用到图像分类中的？请简述其核心流程（分块、编码、注意力机制等）。
  答案：1. 核心思路：Transformer 原本用于自然语言处理（NLP），ViT 的核心创新是将图像“序列化”（转化为类似文本 token 的图像 token），直接复用 Transformer 的编码器架构进行图像分类，摆脱了 CNN 对局部卷积的依赖，能更好地捕捉图像的全局特征。

2. 核心流程：
   （1）图像分块（Patch Embedding）：将输入图像（如 224×224×3）分割为不重叠的固定大小补丁（Patch），如 16×16 的补丁，共(224/16)×(224/16)=196 个补丁；每个补丁通过线性投影（1×1 卷积或全连接层）转化为固定维度的向量（如 768 维），得到 196 个图像 token（Patch Embedding），维度为 196×768。
   （2）添加位置编码（Positional Encoding）：Transformer 编码器没有 CNN 的局部结构归纳偏置，无法感知 token 的空间位置，因此需要为每个图像 token 添加位置编码（可学习的参数或固定的正弦编码），编码向量维度与图像 token 一致（768 维），叠加后得到包含位置信息的 token 序列，维度仍为 196×768。
   （3）添加分类 token（Class Token）：在 196 个图像 token 前添加一个可学习的“分类 token”（[CLS]），用于最终的类别预测，此时 token 序列长度变为 197（196+1），维度 197×768。
   （4）Transformer 编码器编码：将上述 token 序列输入 Transformer 编码器，编码器由多个（如 12 个）相同的编码器层堆叠而成，每个编码器层包含两个核心模块：

- 多头自注意力机制（Multi-Head Self-Attention, MHSA）：计算每个 token 与所有其他 token 的注意力权重，实现全局特征交互（如某个补丁的特征可关注到图像其他区域的补丁特征），捕捉长距离依赖；
- 前馈神经网络（Feed-Forward Network, FFN）：对每个 token 进行独立的非线性变换，增强模型的非线性表达能力。
  （5）分类预测：编码器输出后，仅提取“分类 token”对应的输出向量（768 维），通过一个全连接层（分类头）映射到预定义的类别数（如 1000 类），经 Softmax 激活得到类别概率分布，完成图像分类。

实践应用类

- 在实际项目中，如何选择合适的图像分类模型（如考虑模型大小、推理速度、准确率、硬件资源等）？
  答案：实际项目中选择图像分类模型需结合“业务需求、硬件条件、性能指标”三者平衡，核心选择思路如下：

1. 明确核心性能优先级：
   （1）若优先级为“准确率”（如医疗图像分类、高精度物体识别）：优先选择深层、复杂模型，如 ResNet152、DenseNet201、ViT-L/14、Swin Transformer-L 等，可配合迁移学习、微调提升性能；
   （2）若优先级为“推理速度”（如实时监控、移动端 APP、边缘设备）：优先选择轻量级模型，如 MobileNet 系列（MobileNetV3-Small）、ShuffleNet 系列、EfficientNet-Lite 系列、SqueezeNet 等；
   （3）若优先级为“模型大小”（如嵌入式设备、存储受限场景）：选择参数少的轻量级模型，可进一步通过模型量化（INT8）、裁剪压缩模型体积。
2. 匹配硬件资源：
   （1）云端/服务器（GPU 算力充足）：可选择大型模型（如 ViT、Swin Transformer），支持批量推理，提升吞吐量；
   （2）移动端/边缘设备（CPU/GPU 算力有限）：必须选择轻量级模型，避免模型过大导致无法部署或推理卡顿，优先选择针对移动端优化的模型（如 MobileNet、MT-CNN）；
   （3）专用芯片（如 NPU、FPGA）：选择适配芯片架构的模型（如华为昇腾芯片适配 MindSpore 框架下的模型），提升硬件利用率。
3. 结合数据集特点：
   （1）小数据集/少样本场景：优先使用迁移学习，选择在大规模数据集（如 ImageNet）上预训练的模型（如 ResNet50、ViT-B/16），通过微调适配目标任务，避免从头训练导致过拟合；
   （2）大数据集/复杂场景（如多类别、多姿态、多光照）：选择表达能力强的模型（如 Swin Transformer、EfficientNet-B7），充分利用数据优势提升性能；
   （3）类别不平衡数据集：选择对不平衡数据鲁棒的模型，或配合数据增强、损失函数优化（如 Focal Loss），无需过度追求模型复杂度。
4. 工程落地成本：
   （1）选择开源成熟的模型（如 PyTorch/TensorFlow 官方实现），减少自研成本；
   （2）考虑模型部署框架的兼容性（如 ONNX、TensorRT 对模型的支持程度），避免部署困难。

- 当图像分类模型在测试集上准确率较低时，你会从哪些方面进行问题排查和优化？
  答案：模型测试集准确率低，核心原因分为“欠拟合”“过拟合”“数据问题”“模型设计问题”四类，排查和优化需按“数据 → 模型 → 训练策略 → 工程细节”的顺序逐步推进：

1. 排查数据问题（最优先，数据决定模型上限）：
   （1）数据质量：检查测试集是否存在标注错误（如类别标错、漏标）、图像模糊/损坏/噪声过多；训练集与测试集是否存在分布差异（如训练集是白天图像，测试集是夜晚图像），可通过可视化样本分布验证；
   （2）数据量：若训练集样本过少（如每类不足 100 张），模型无法学习到通用特征，导致泛化差，需补充数据或加强数据增强（如 MixUp、CutMix）；
   （3）类别平衡：检查是否存在严重类别不平衡（如少数类样本占比＜ 5%），导致模型偏向多数类，可通过过采样少数类、欠采样多数类、加权损失函数（如 Weighted Cross-Entropy）优化。
2. 排查模型设计问题：
   （1）模型复杂度：若模型过简单（如 LeNet 用于复杂场景），存在欠拟合（训练集准确率也低），需更换更复杂的模型（如 ResNet50）或加深/加宽现有模型；若模型过复杂（如 ViT 用于简单数据集），存在过拟合（训练集准确率高，测试集低），需简化模型或增加正则化；
   （2）特征提取能力：检查模型是否能有效提取目标特征，可通过可视化卷积层输出（如 Grad-CAM）查看模型关注区域，若关注无关区域（如背景），需调整模型（如增加注意力机制）或优化数据；
   （3）输出层匹配：确保输出层神经元数与类别数一致，激活函数选择正确（多分类用 Softmax，二分类用 Sigmoid）。
3. 排查训练策略问题：
   （1）超参数设置：学习率过高（模型震荡不收敛）或过低（收敛慢、停留在局部最优），可通过学习率调度器（如 Cosine Annealing）调整；批次大小（Batch Size）过小（梯度波动大）或过大（内存不足、梯度平滑），需匹配硬件资源；正则化强度（如 Dropout 概率、L2 权重衰减）不当，需调整参数；
   （2）训练过程：检查是否训练不足（损失未收敛），需增加训练轮数；是否存在梯度消失/爆炸，可通过 BN、梯度裁剪、残差连接优化；
   （3）迁移学习策略：若使用预训练模型，检查微调策略是否合理（如是否冻结底层、学习率是否适配微调），避免预训练特征被破坏。
4. 工程细节排查：
   （1）数据预处理：训练集与测试集的预处理逻辑是否一致（如归一化均值/方差、图像尺寸是否相同），若测试集未归一化或尺寸错误，会导致准确率骤降；
   （2）模型保存与加载：检查是否加载了训练过程中泛化最优的模型（如早停保存的模型），避免加载了训练最后一轮的过拟合模型；
   （3）计算资源：训练过程中是否存在硬件性能不足（如 GPU 显存溢出导致模型截断），需优化模型或降低批次大小。

- 请说明迁移学习在图像分类中的应用场景和具体方法（如预训练模型选择、微调策略等）。
  答案：1. 迁移学习核心思想：将在“源任务”（如 ImageNet 1000 类分类）上训练好的模型参数（预训练模型）迁移到“目标任务”（如自定义的猫狗分类），利用源任务学习到的通用特征（如边缘、纹理、目标部件），提升目标任务的模型性能，减少目标任务的训练数据量和训练成本。

2. 应用场景：
   （1）目标任务数据集小（少样本场景）：如医疗图像分类（数据标注成本高）、小众类别分类（如特定植物/动物分类），此时从头训练模型易过拟合，迁移学习可有效提升泛化能力；
   （2）目标任务与源任务相似度高：如源任务是自然场景物体分类，目标任务是街景物体分类，通用特征可直接复用，迁移效果好；
   （3）快速模型迭代：工程落地中需快速上线模型，迁移学习可大幅缩短训练时间（无需从头训练深层网络）；
   （4）资源受限场景：边缘设备、移动端训练资源有限，迁移学习可减少训练算力需求。
3. 具体方法：
   （1）预训练模型选择：

- 优先选择在大规模、通用数据集上预训练的模型（如 ImageNet、COCO），这类模型学习的特征更通用；
- 匹配目标任务复杂度：简单任务（如二分类）选择轻量级预训练模型（如 MobileNetV3），复杂任务（如多类别精细分类）选择深层模型（如 ResNet101、ViT）；
- 匹配图像尺寸：目标任务图像尺寸与预训练模型输入尺寸相近（如 ViT 预训练输入 224×224，目标任务也尽量用 224×224），减少特征错位。
  （2）微调策略（核心步骤）：
- 冻结底层，微调顶层：将预训练模型的底层卷积层（前几层）参数冻结（不更新），仅解冻顶层全连接层（分类头），替换为适配目标任务类别数的新分类头，训练时仅更新顶层参数。适用场景：目标数据集极小，底层通用特征可直接复用（如边缘、纹理）。
- 逐步解冻微调：先冻结底层训练顶层，待顶层收敛后，解冻部分中层卷积层（如 ResNet 的后 4 层），用更小的学习率（如初始学习率的 1/10）继续训练。适用场景：目标数据集中等规模，需要微调中层特征适配目标任务。
- 全量微调：解冻所有层参数，用极小的学习率（如 1e-5~1e-4）训练整个模型。适用场景：目标数据集规模大（如 10 万+样本），与源任务相似度高，需充分适配目标特征。
  （3）其他辅助方法：
- 数据增强：配合迁移学习使用，进一步提升泛化能力（如 MixUp、CutOut）；
- 学习率调度：微调时使用小学习率，避免破坏预训练的通用特征，可使用余弦退火、阶梯式下降（StepLR）；
- 正则化：添加 Dropout、L2 权重衰减，防止微调过程中过拟合。

- 如何处理图像分类任务中的类别不平衡问题？请列举至少 3 种解决方案，并说明其适用情况。
  答案：类别不平衡指训练集中不同类别的样本数量差异显著（如多数类样本占比 90%以上，少数类占比不足 10%），导致模型偏向多数类，少数类识别准确率低。常见解决方案及适用情况如下：

1. 数据层面解决方案：
   （1）过采样（Oversampling）：增加少数类样本数量，使各类别样本均衡。常用方法：随机过采样（直接复制少数类样本）、SMOTE（合成少数类样本，通过插值生成新的少数类样本，如取两个少数类样本的特征均值作为新样本）。
   适用情况：少数类样本极少（如每类＜ 50 张），且样本多样性不足；不适用情况：少数类样本本身存在噪声，过采样会放大噪声，导致过拟合。
   （2）欠采样（Undersampling）：减少多数类样本数量，使各类别样本均衡。常用方法：随机欠采样（随机删除多数类样本）、Easy Ensemble（多次随机欠采样生成多个子集，训练多个模型集成）。
   适用情况：多数类样本极多（如百万级），训练成本高；不适用情况：多数类样本中包含重要特征，欠采样可能丢失关键信息，导致模型泛化能力下降。
2. 损失函数层面解决方案：
   （1）加权交叉熵损失（Weighted Cross-Entropy）：为不同类别分配不同的权重，少数类权重高，多数类权重低，使模型训练时更关注少数类。公式：Loss = -Σ(w_i × y_i × log(p_i))，其中 w_i 是第 i 类的权重（少数类 w_i ＞ 1，多数类 w_i ＜ 1）。
   适用情况：各类别样本差异适中（如少数类占比 10%-30%），简单易实现，通用性强；不适用情况：权重设置需手动调优，权重过大可能导致少数类过拟合。
   （2）Focal Loss：通过引入“调制因子”（1-p_t)^γ（p_t 是模型对正确类别的预测概率），降低易分类样本（多数类样本通常易分类）的权重，提升难分类样本（少数类样本通常难分类）的权重。公式：Loss = -α_t × (1-p_t)^γ × log(p_t)，α_t 是类别权重，γ 是聚焦参数（γ≥0）。
   适用情况：类别不平衡严重（少数类占比＜ 10%），且存在大量易分类样本；不适用情况：γ 参数需要调优，γ 过大会导致模型训练不稳定。
3. 模型层面解决方案：
   （1）迁移学习/元学习：用少数类样本进行微调，或通过元学习快速适配少数类特征。适用情况：少数类样本极少，且存在相似的源任务预训练模型（如用 ImageNet 预训练模型微调医学少数类疾病图像）。
   （2）集成学习：训练多个基础模型（如多个 CNN），每个模型侧重不同类别的学习（如部分模型侧重少数类），最后通过投票、加权融合输出结果。适用情况：类别不平衡严重，单一模型效果差；不适用情况：训练成本高，工程落地复杂。
4. 其他辅助方案：
   数据增强：针对少数类样本进行更激进的数据增强（如更多角度的旋转、更多样的色域变换），增加少数类样本多样性，无需增加真实样本数量。适用情况：所有类别不平衡场景，尤其适合少数类样本有一定基础（如每类＞ 50 张）的情况。

二、目标检测相关面试题

基础概念类

- 请简述目标检测的定义和核心任务，与图像分类相比，它的难点在哪里？
  答案：1. 定义与核心任务：目标检测是计算机视觉的核心任务之一，指给定一张输入图像，模型需要同时完成两个核心任务：（1）定位（Localization）：找出图像中所有目标的位置，通常用边界框（Bounding Box，如 x1,y1,x2,y2 坐标）表示；（2）分类（Classification）：判断每个定位到的目标所属的预定义类别（如人、车、狗）。最终输出为“目标类别+边界框坐标”的组合结果。

2. 与图像分类的难点对比：图像分类仅需判断图像整体类别，无需关注目标位置，而目标检测需同时解决“定位+分类”，难点更突出，具体包括：
   （1）目标位置不确定性：图像中目标的数量、位置、尺度、姿态均不固定（如一张图可能有 1 个或 10 个目标，目标可能大或小、正立或倾斜），模型需适应这种多样性；
   （2）背景复杂度：目标可能被遮挡（部分遮挡或完全遮挡）、与背景颜色相似（如白色猫在雪地），导致目标特征不完整或被背景干扰，增加定位和分类难度；
   （3）多目标重叠：多个目标可能相互重叠，边界框难以准确划分，容易出现漏检或误检；
   （4）小目标检测难度高：小目标像素少、特征信息有限，模型难以提取有效特征，容易被当成背景或噪声忽略；
   （5）精度与速度的平衡：目标检测常需实时性（如自动驾驶、监控），需在保证定位和分类精度的同时，提升模型推理速度，比图像分类的工程要求更高；
   （6）评估指标更复杂：图像分类用准确率、F1 等指标即可，目标检测需用 mAP、FPS 等综合评估定位精度和推理速度，指标优化难度更大。

- 什么是锚点（Anchor）？锚点的设计原则是什么？不同尺度、不同长宽比的锚点对检测效果有什么影响？
  答案：1. 锚点定义：锚点（Anchor，也叫锚框、预设框）是目标检测算法（如 Faster R-CNN、YOLOv3、SSD）中预先定义在图像特征图上的一系列固定大小、固定长宽比的边界框。其核心作用是为模型提供“候选目标位置”，帮助模型快速定位目标，减少目标位置的搜索空间，提升检测效率。
  注意：锚点是定义在特征图上的，需通过“缩放因子”映射回原始图像，得到原始图像上的候选边界框。

2. 锚点设计原则：
   （1）覆盖目标尺度：锚点的尺度（大小）需覆盖数据集中目标的主要尺度范围（如数据集包含小目标、中目标、大目标，需设计对应尺度的锚点），避免锚点尺度与目标尺度差异过大，导致模型无法匹配；
   （2）匹配目标长宽比：锚点的长宽比需匹配数据集中目标的常见长宽比（如人是高瘦型，长宽比 2:1；车是矮胖型，长宽比 1:2；通用目标用 1:1），减少锚点与目标边界框的形状差异；
   （3）密度适配：目标密集区域（如人群、车流）需设计更密集的锚点，避免锚点覆盖不足；目标稀疏区域可减少锚点，降低计算量；
   （4）与特征图层级匹配：深层特征图感受野大，适合匹配大尺度锚点（检测大目标）；浅层特征图感受野小，适合匹配小尺度锚点（检测小目标）（如 SSD、YOLOv3 的多尺度锚点设计）。
3. 不同尺度、长宽比的锚点对检测效果的影响：
   （1）尺度影响：

- 锚点尺度过小：无法覆盖大目标，导致大目标漏检；
- 锚点尺度过大：小目标被锚点包含过多背景，模型难以学习小目标特征，导致小目标漏检或误检；
- 尺度覆盖不全面：仅设计单一尺度锚点，无法适配数据集中多尺度目标，整体检测精度下降。
  （2）长宽比影响：
- 长宽比与目标不匹配：锚点与目标边界框的 IoU（交并比）偏低，模型难以将锚点调整为准确的目标边界框，导致定位精度低；
- 长宽比数量过多：增加锚点总数，提升计算量，可能导致过拟合；
- 长宽比数量过少：无法覆盖异形目标（如细长的电线杆、扁平的桌子），导致这类目标漏检。

- 请解释 IOU（Intersection over Union）的定义和计算方式，它在目标检测中的作用是什么？除了 IOU，还有哪些改进的评价指标（如 GIoU、DIoU、CIoU），它们分别解决了什么问题？
  答案：1. IOU（交并比）：
  （1）定义：两个边界框（预测框 P、真实框 G）的交集面积与并集面积的比值，用于衡量两个框的重叠程度，取值范围[0,1]，值越接近 1 表示重叠度越高，匹配效果越好。
  （2）计算方式：IOU = Area(P ∩ G) / Area(P ∪ G)，其中 Area(P ∩ G)是预测框与真实框的交集面积，Area(P ∪ G)是并集面积（并集面积=预测框面积+真实框面积-交集面积）。
  （3）在目标检测中的作用：
- 锚点匹配：训练时，用 IOU 判断锚点与真实框的匹配程度（如 IOU≥0.5 视为正样本，IOU≤0.1 视为负样本），为模型提供训练标签；
- 边界框回归损失：早期目标检测用 IOU 作为回归损失的一部分，引导模型优化预测框位置；
- 评估指标：作为目标检测评估的核心指标（如 mAP 计算中需用到 IOU 阈值），判断预测框的定位精度（如 IOU≥0.5 视为正确定位）。

2. 改进指标及解决的问题（IOU 的局限性：当两个框无重叠时，IOU=0，无法区分距离远近；仅考虑重叠区域，未考虑框的位置、大小关系）：
   （1）GIoU（Generalized IOU，广义交并比）：

- 定义：GIoU = IOU - (Area(C) - Area(P ∪ G))/Area(C)，其中 C 是能同时包含 P 和 G 的最小包围框；
- 解决问题：① 两个框无重叠时，GIoU≠0，能反映两个框的距离（距离越远，GIoU 越小），为模型提供梯度（IOU 此时梯度为 0，无法优化）；② 考虑了两个框的空间位置关系，而非仅重叠区域。
  （2）DIoU（Distance IOU，距离交并比）：
- 定义：DIoU = IOU - (d²)/(c²)，其中 d 是 P 和 G 的中心点欧氏距离，c 是包围框 C 的对角线长度；
- 解决问题：① 相比 GIoU，更直接地优化两个框的中心点距离，收敛速度更快；② GIoU 在两个框处于包含关系时（如 P 完全在 G 内），退化为 IOU，无法进一步优化，而 DIoU 仍能通过中心点距离优化定位精度。
  （3）CIoU（Complete IOU，完全交并比）：
- 定义：CIoU = IOU - (d²)/(c²) - αv，其中 α 是权重系数，v 是衡量 P 和 G 长宽比一致性的指标（v = 4/π² × (arctan(w_G/h_G) - arctan(w_P/h_P))²）；
- 解决问题：在 DIoU 的基础上，增加了“长宽比一致性”的约束，同时优化了中心点距离、重叠度、长宽比三个维度，使预测框更快速、准确地逼近真实框，尤其适合对边界框形状要求高的场景。

- 什么是 NMS（非极大值抑制）？请简述其原理和步骤，它在目标检测中用于解决什么问题？常见的 NMS 改进方法有哪些？

- 目标检测任务的常用评估指标 mAP（mean Average Precision）是如何计算的？请详细说明 AP 的计算流程。

经典算法原理类

- 请简述目标检测算法的两大流派（两阶段检测算法、一阶段检测算法）的核心区别和各自的优缺点。

- R-CNN 的核心流程是什么？它存在哪些不足？Fast R-CNN 是如何对其进行改进的？

- Faster R-CNN 中的 RPN（Region Proposal Network）是如何工作的？它如何实现端到端的目标检测？

- YOLO 系列算法（以 YOLOv3 为例）的核心思想是什么？请简述其网络结构和检测流程。

- YOLOv4、YOLOv5 在 YOLOv3 的基础上做了哪些关键改进？（如 Backbone、Neck、Head 的改进，数据增强、损失函数优化等）

- SSD（Single Shot MultiBox Detector）是如何实现多尺度目标检测的？它与 YOLO 相比，在检测小目标上有什么优势？

- RetinaNet 是如何解决目标检测中的类别不平衡问题的？请详细说明 Focal Loss 的原理和作用。

- DETR（Detection Transformer）的核心创新点是什么？它如何摆脱对锚点的依赖？

实践应用类

- 在实际项目中，如何优化目标检测模型的推理速度？请列举至少 4 种方法，并说明其原理。

- 当目标检测模型出现漏检、误检问题时，你会从哪些方面进行优化？（如数据、模型、参数等维度）

- 小目标检测的难点是什么？请列举至少 3 种针对小目标检测的优化方案。

- 请说明目标检测模型在部署时需要注意哪些问题？（如模型量化、裁剪、框架选择等）

三、图像分割相关面试题

基础概念类

- 请简述图像分割的定义，以及语义分割、实例分割、全景分割的核心区别和应用场景。

- 图像分割任务的常用评估指标有哪些？请解释 Dice 系数、IoU、Pixel Accuracy、Mean IoU 的计算方式和适用场景。

- 请说明图像分割与图像分类、目标检测的本质区别，其技术难点在哪里？

经典算法原理类

- FCN（Fully Convolutional Networks）是如何将 CNN 应用到图像分割中的？请简述其核心思想和网络结构（如转置卷积、跳跃连接的作用）。

- U-Net 的网络结构有什么特点？它在医学图像分割中
